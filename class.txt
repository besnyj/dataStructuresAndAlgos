Overview of Arrays and Memory:

- Arrays are collection of items of a single type. Integers, strings, etc.
- Memory is temporary and storage is permanent

Classes and Objects:

- Classes could be understood as a template (blueprint) definition of the methods and variables in a particular kind of object.
- Objectas are like "variables" with unique attributes and behaviors. They try to represent real objects from the real world.
- attributes from an object my be called instance variables and function within an object is called methods.

Intodruction to Linked Lists:

- data structure to store a collection of items
ex:
- class Box: *Should call Node instead of box
	head.data - refers to the data inside the given box
	head.next - refers to the next box
	head.next.data - refers to the data in the next box
	head.next.next - refers to the next next box
- and this is why they are called linked list, because you can only go into one direction.
- can also create a doubly linked list by adding the "previous" attributes.
more ex on how to count nodes in a linked list:
class Node:
    def __init__(self, data):
        self.data = data
        self.next = None


head = Node(1)
nodeB = Node(2)
nodeC = Node(3)

head.next = nodeB
nodeB.next = nodeC


def countnodes(head):
    i = 0
    node = head
    while True:
        try:
            node = node.next
            i += 1
        except:
            return i

def countnodes2(head):

    j = 0
    node = head
    while True:
        if node.next != None:
            j+=1
            node = node.next
        if node.next == None:
            j+=1
            return j

def countnodes3(head):
    k = 1
    node = head
    while node.next != None:
        node = node.next
        k +=1
    return k

print(countnodes(head))
print(countnodes2(head))
print(countnodes3(head))

Introduction to Recursion:

- a way of solving a problem by having a function call itself.

Ex:

def fact(n):

    if n >= 1:
        return n*fact(n-1)
    else:
        return 1

print(fact(50))

another example but with Fibonacci sequence:

def fib(n):
    if n >= 3:
        return fib(n-1)+fib(n-2)
    else:
        return 1
print(fib(8))

- we can also implement dynamic programming to help or recursion
	1. Recursion
	2. Store (memoize)
	3. Bottom up
	
	Notice how in the first example, we have to calculate the fib(2) 3 times, fib(3) 2 times and etc.
	So, we can store the return value for fib(2), so if we need to calculate i again, we can skip
		the calculation and go directly to the result, saving us time and memory space

Memoized Solution:
	
	we are going to create an array of len(n) to store the results of fib(n) and initially we are goin g to set all of the values to None.

# need to initialize the array before calling the function
def fib(n, memo)
	if memo[n] != None:
		return memo[n]
	if n == 1 or n == 2
		result = 1
	else:
		return == fib(n-1) + fib(n-2)
	memo[n] = result
	return result

	Bottom up approach:
		we are creating an array called bottom up that stores the values from left to right
		
	
	def fib_bottom_up(n):
		if n == 1 or n == 2:
			return 1
		bottom_up = [n+1]
		bottom_up[1] = 1
		bottom_up[2] = 1
		for i from 3 upto n:
			bottom_up[i] = bottom_up[i-1]+bottom_up[i-2]
		return bottom_up[n]

EX of how to do it in python:

def num_ways(n):
    if n == 0 or n == 1:
        return 1

    bottom_up = [0]*(n+1)

    bottom_up[0] = 1
    bottom_up[1] = 1

    for i in range(2, n+1):
        bottom_up[i] = bottom_up[i-1]+bottom_up[i-2]
    print(bottom_up)

    return bottom_up[n]

print(num_ways(3))

Introduction to Big O Notation and Time Complexity:

- time it takes to run the size of the input to your function:
- how does the runtime of this function grow?
- to define the runtime of space and time, we use Big O Notation and Time Complexity
- Linear time = O(n), constant time = O(1), quadratic time = O(nˆ2).
- how to find out the runtime complexity:
    - find the fastest growing term
    - take out the coefficient
- one advantage of using big O is finding the result independent of the environment
- allow you to quickly compare multiple functions in terms of their performances
- steps to find bigO again:
    - checking the bigO of each line or process, for ex:
        def stupid_function(array):
            total = 0 -> this line is O(1)
            return total -> this line is also O(1)
    now, add them up and check what is the big O for it:
    C = constant
        T = O(1) + O(1) = c1 + c2, therefore = c3 = c3*1 which is equal to O(1).
        It means that if you only have O(1), the runtime is gonna be O(1)
    - another EX:
        def find_sum(array):
        total = 0 -> O(1)
        for each i in array: -> O(1)
            total += i -> O(1)
        return total -> O(1)
        T = O(1) + n*O(1) + O(1)
        T = O(n)
    - another EX:
    array_2d = [[1,4,3],
                [3,1,9],
                [0,5,2]]
    def find_sum_2d(array_2d):
        total = 0 -> O(1)
        for each row in array_2d: -> double for loop is nˆ2*O(1)
            for each i in row:
                total += i O(1)
        return total O(1)
        T = O(1) + nˆ2*O(1) + O(1)
        T = c + nˆ2*c
        T = O(nˆ2)

- exercises:
        int product(int a, int b) {
        int sum = 0; -> O(1)
        for (int I = 0; I < b; I++) { -> O(n)
            sum += a; -> O(1)
        }
        return sum; -> O(1)
    }

    T = O(1) + O(1) + O(1) + n*O(1)
    T = c1 + c2 + c3 + n*c4
    T = O(n), linear runtime

    -----------------------------------
        static int power(int a, int b) {
        if (b < 0) return a; -> O(1)
        if (b == 0) return 1; -> O(1)
        int sum = a; -> O(1)
        for (int I = 0; I < b - 1; I++) { -> O(n)
            sum *= a; -> O(1)
        }
        return sum; -> O(1)
    }

    T = O(1) + O(1) + O(1) + n*O(1) + O(1)
    T = c1 + c2 + c3 + n*c4 + c5
    T = O(n), linear runtime

    ------------------------------------
        int mod(int a, int b) {
        if (b <=a) return -1; -> O(1)
        int div = a / b; -> O(1)
        return a - div * b; - O(1)
    }

    T = O(1) + O(1) + O(1)
    T = c1 + c2 + c3
    T = O(1), constant runtime

    -------------------------------------
        int div(int a, int b) {
        int count = a; -> O(1)
        int sum = b; -> O(1)
        while (sum <= a) { -> O(n)
            sum += b; -> O(1)
            count++; -> O(1)
        }
        return count; - O(1)
    }

    T = O(n), linear runtime

    ------------------------------------
        int sqrt(int n) {
        return sqrt_helper(n, 1, n);
    }
    int sqrt_helper(int n, int min, int max) {
        if (max < min) return -1;
        int guess = (min + max) / 2;
        if (guess * guess == n) {
            return guess;
        } else if (guess *guess <n) {
            return sqrt_helper(n, guess + 1, max) ;
        } else {
            return sqrt_helper(n, min, guess - 1);
        }
    }

    T = O(Log N) because it is doing essentially what a binary tree would do.

    --------------------------------------

        int[] copyArray(int[] array) {
        int[] copy = new int[0];
        for (int value : array) {
            copy = appendToNew(copy, value); -> this for loop O(n) calls another function with a for loop,
        }                                       there fore nˆ2*O(1), therefore O(nˆ2)
        return copy;
    }
    int[] appendToNew(int[] array, int value) {
        int[] bigger = new int[array.length + 1];
        for (int I = 0; I < array. length; I++) {
            bigger[I] = array[I];
        }
        bigger[bigger.length - 1] = value;
        return bigger;
    }

    T = O(nˆ2)

    ----------------------------------------
        int fib(int n) {
        if (n <= 0) return 0;
        if (n == 1) return 1;
        return fib(n - 1) + fib(n - 2);
    }

    algorithm with multiple recursive calls, therefore O(branchesˆdepth), O(2ˆn)

Introduction  to Trees:

- trees are linked lists with only 1 reference that link to each node
- tree is a structure where you can go from the root node to any other node, also no other node refers to the root

class Node:
    data
    Node left
    Node right

- now, let's sum all the data in the nodes with a O(n) runtime

def find_sum(root):
    if root == None:
        return 0
    return root.data + find_sum(root.left) + find_sum(root.right)

- non-empty universal value trees: tree which values are all the same

Introduction to Binary Search:

- searching algorithm used in a sorted array by repeatedly dividing the search interval in half

Ex: Let's set an array = [-2,3,4,7,8,9,11,13] and a target of 11. We should create an algorithm
    that return the index of the target in the given array. Doing on a linear search algorithm,
    we could write it like this:

        def search(arr, target):

        for index, value in enumerate(arr):
            if value == target:
                return index
        return -1

    However, this solution has a O(n) runtime complexity. Then, let's make it a O(n/2), or, binary search:

    Ex: we have an array [-50....50], target 20.

    - 1st step: if the target exists, it is between the first and last element:
        Then, we create two pointers: L and R.
    - 2nd step: check the number in the middle of the array.
        Because the number in the middle is 0, we can move our L pointer to 11
        Now, we redo the process. However, if the middle number is larger than our target, we move R to the middle number.

- time complexity of the given approach:
    O(log2n) = O(logn)
    therefore if we have 10,000,000 items in our array:
    log2(10,000,000) ~ 24. So at most, we'll have only to check 24 numbers.

    now let's code the solution:
    Let's set an array = [-2,3,4,7,8,9,11,13] and a target of 11

def search(arr, target):
    left = 0  ## index fo the pointer
    right = len(arr)-1 ## index of the pointer
    while left <= right:
        mid = (left + right)/2 ## if it is not an integer, round it DOWN.
                               ## the result is the INDEX of the number in the middle, to the number itself
        if arr[mid] == target:
            return mid
        if target < arr[mid]:
            right = mid-1
        else:
            left = mid+1
    return -1 ## to show the target doesnt exist in the array.

Complete Overview of Quicksort:

- it is the so-called divide and conquer algorith. It selects a pivot element from the array and
partion the other elements into two sub arrays, according to whether they are less than or greater
than the pivot.
- also called partition-exchange sort.
- it implements a recursive function that implements 3 arguments.
- doesnt work pretty well with many duplicates in the array

EX: let's say you want to sort the following array [3,-2,-1,0,2,4,1] into [-2,-1,0,1,2,3,4]

    - 1st step: set two pointers, L and R. L is at the first index and R is at the last index
    - 2nd step: basecases, l >= r means the section of the array has only 1 element, then return
    - pick partition and pivot
    - in our case, if we select number 1, index 6, will be our pivot. It means that
      every number bigger ou equal than our pivot will come after 1 and smaller will come before.
    at the partition function, you should swap numbers in two sub arrays, the greater than the pivot
    and smaller than the pivot. Then, return the index of the pivot in the new sorted array.

arr = [-2,3,-1,5,4,-3,0]
l = arr[0]
r = arr[6]

def partition(arr, l, r): # this is written in pseudocode
    pivot = arr[r] ## this sets the last element of the array as the pivot.
    i = l - 1
    for j from l upto r - 1:
        if arr[j] < pivot:
            i += 1
            swap arr[1] and arr[j]
    swap arr[i+1] and arr[r]
    return i + 1

def qs(arr, l, r):
    if l >= r:
        return
    p = partition(arr, l, r) # will set out pivot, then we will have to sort what comes before and after

    qs(arr, l, p-1)
    qs(arr, p+1, r)

Time complexity of partition:
- Worst case O(nˆ2)
- Average case O(n)
- Best case O(nlogn)

- implementation details
    - Choosing pivot:
        pick a random element
        pick the last element of the array
        median of three: pick 3 random and pick the median of these three, reducing the chance of a bad pick
    - Dealing with duplicates
        3-way quicksort

Introduction to Stack and Queues:

- Stacks are like pancakes, they work as a last in first out system.
- it is a collection of data in which we can only add a piece of data on the top of
    the structure and retrieve data only from the top.
- can be implemented using an array and using pointers.
- a queue, as seen before, works as a real queue, first in first out system.
- also implemented with an array, but now uses two pointers. However, when using stacks and having
    to store more elements than the lenght of the satck, you will have to create another stack.
    In the case of queue, the next item to be added will be allocated at the beginning of the array.
- both of the data structures are O(1)
- deque/double ended queue: can remove elements on the two points and also add elements at the beginning and
    the end of the array.

Introduction to Hash Tables and Dictionaries:

- Dict is a structure of data made of key/value pairs, so that we can retrieve the value of the keys quickly.
- We can also define more operations.
    - insert: adding a new key/value.
    - delete: self explanatory.
    - search.
    * we need to implement these features with a O(1) complexity.

- Building a hash table:

genericDict = {
    "Paul":"29",
    "Jane":"35",
    "Chloe":"88",
    "Alex":"18"
}

- Now, we have to decide the position of our key/values in an array. Let's take as example an array of lenght 8:

[_,_,_,_,_,_,_,_]

- As we have the array, we can arrange our key/values as we wish. The third column represents the position of
    each k/v in the array:

genericDict = {
    "Paul":"29", 7,
    "Jane":"35", 1,
    "Chloe":"88", 2,
    "Alex":"18", 0
}

- Therefore, our array is now a hashtable:

[Alex 18, Jane 35, Chloe 88,_,_,_,_Paul 29]

- To decide the position of the items in the array, we create a hash function.
    - needs to be fast to compute.
    - avoid collisions (more than 1 key/value in the same bucket)

- Chaining: instead of storing the items in an array, we store them in a linked link and from each element in the
    array we will have a pointer to the next item in the linked list.
Ex: [_,Jane 38,_,....]
    [_,Jane 38->Chloe 88->Alex 18, ....]
- Insertion: O(1)
- search = O(1+a), a = length of the array/elements in the array

- Open addressing:
    - linear probing:
        - store the value in a linear way. When we have a collision, we check the element to the right subsequently
        until we find it empty, and then we fill it with the colliding element.
        * however, if we have a cluster, it is going to take much time to compute.
            for this problem, we use double hashing:
    - instead of jumping ahead to the right, we piuck a number to check how many elements ahead we want.
        Ex: if we pick 3, we will always check the third element after the collision point. If it doesnt
            exist, we jump back to the beginning of the list.

- i = h1(key) mod 8
- (i + c) mod 8
- (i + 2c) mod 8

- greatest common divider: gcd(c, m) = 1
- always set M (length of the arr) as a prime number, and C a positive integer less than M
- c <- {h2(key) mod(m-1)} + 1
- h2(key) = h1(key+'d')
- h2(key) = h1(key)

- search = (1/(1-a))
- insertion (1/(1-a))

*** Algorithms to study next ***

* What does transverse mean

1. Depth First Search, sort used to solve tree or graph structure problems.
2. Breadth First Search, differs from the 1. by using opposite method with a queue.
3. Matching Parenthesis, this is in almost every single interview.
4. Hash Tables, useful for matrix and for keeping track of what you already visited.
5. How to manipulate variables/pointers at once. Ex: longest palindome number in an string.
6. Reversing Link List, needs 3 different pointers.
7. Sorting Fundamentals, sorting algorithms
8. Recursion. Already learned but check again.
9. Custom Data structures.
10. Binary Search. Also already learned, but it is good to review this topic


